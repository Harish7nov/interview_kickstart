{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb6b300",
   "metadata": {},
   "source": [
    "# evaluate the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import multiprocessing\n",
    "import json\n",
    "import time\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a55a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the base model\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token, tokenizer.unk_token)\n",
    "print(tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b77da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for smollm 360m\n",
    "# set the pad token as eos token so that the finetuned model knows when to stop generating tokens\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df0d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token, tokenizer.unk_token)\n",
    "print(tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lora_weights/checkpoint-500\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_path, device_map=\"cpu\", torch_dtype=torch.float16)\n",
    "\n",
    "# * merge the adapter onto the base model itself\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "print(merged_model.num_parameters() / 1e6, \" M Params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the valid dataset\n",
    "valid_data = []\n",
    "with open('data/valid.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        valid_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[1][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732833a",
   "metadata": {},
   "source": [
    "# single inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 15\n",
    "\n",
    "input_text = f\"<user>{valid_data[ind]['event_text']}</user><output>\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(f\"\\nInput Text : {input_text}\")\n",
    "print(f\"Length of input tokens processed : {len(inputs['input_ids'][0])}\")\n",
    "\n",
    "output_tokens = merged_model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"Length of output tokens generated : {len(output_tokens[0]) - len(inputs['input_ids'][0])}\")\n",
    "\n",
    "truncated_output = tokenizer.decode(output_tokens[0][len(inputs[0]):])\n",
    "output = tokenizer.decode(output_tokens[0])\n",
    "\n",
    "print(f\"Output text : {truncated_output}\")\n",
    "resp = output.split(\"</output>\")[0].split(\"<output>\")[1]\n",
    "\n",
    "pred = json.loads(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a76127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ik_prob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
